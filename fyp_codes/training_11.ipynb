{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04233d7d-218a-49de-bb55-d1f668ec506c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### UNet architecture class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223750a3-b3cb-4a8c-94e4-82c30be410f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        \n",
    "        # Regularization to reduce overfitting\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "       \n",
    "        # ENCODER\n",
    "        # customize each layer of double conv\n",
    "        self.down_conv_1 = self.doubleConv(1,64)\n",
    "        self.down_conv_2 = self.doubleConv(64,128)\n",
    "        self.down_conv_3 = self.doubleConv(128,256)\n",
    "        self.down_conv_4 = self.doubleConv(256,512)\n",
    "        self.down_conv_5 = self.doubleConv(512,1024)\n",
    "        \n",
    "        # DECODER\n",
    "        self.up_conv_1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=(2,2), stride=(2,2))\n",
    "        self.up_conv_2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=(2,2), stride=(2,2))\n",
    "        self.up_conv_3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(2,2), stride=(2,2))\n",
    "        self.up_conv_4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(2,2), stride=(2,2))\n",
    "        \n",
    "        self.down_conv_6 = self.doubleConv(1024,512)\n",
    "        self.down_conv_7 = self.doubleConv(512,256)\n",
    "        self.down_conv_8 = self.doubleConv(256,128)\n",
    "        self.down_conv_9 = self.doubleConv(128,64)\n",
    "\n",
    "        self.down_conv_final = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=(1,1))\n",
    "        \n",
    "    def doubleConv(self, in_c, out_c):\n",
    "        conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_c, out_channels=out_c, kernel_size=(3,3)),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=out_c, out_channels=out_c, kernel_size=(3,3)),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        return conv\n",
    "    \n",
    "    def cropping(self, ori_tensor, target_tensor):\n",
    "        # assuming image is perfect square\n",
    "        ori_tensor_width = ori_tensor.shape[3]\n",
    "        target_tensor_width = target_tensor.shape[3]\n",
    "        delta = ori_tensor_width - target_tensor_width\n",
    "        delta = delta // 2 # assume perfect square\n",
    "        return ori_tensor[:,:,delta:ori_tensor_width-delta, delta:ori_tensor_width-delta]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x1 = self.down_conv_1(x)# output got concatenated\n",
    "        x2 = self.maxpool(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.down_conv_2(x2)# output got concatenated\n",
    "        x4 = self.maxpool(x3)\n",
    "        x4 = self.dropout(x4)\n",
    "        x5 = self.down_conv_3(x4)# output got concatenated\n",
    "        x6 = self.maxpool(x5)\n",
    "        x6 = self.dropout(x6)\n",
    "        x7 = self.down_conv_4(x6)# output got concatenated\n",
    "        x8 = self.maxpool(x7)\n",
    "        x8 = self.dropout(x8)\n",
    "        x9 = self.down_conv_5(x8)\n",
    "        \n",
    "        # decoder\n",
    "        x10 = self.up_conv_1(x9)\n",
    "        x10 = self.dropout(x10)\n",
    "        x11 = torch.cat((x10,self.cropping(x7,x10)), dim=1)\n",
    "\n",
    "        x12 = self.down_conv_6(x11)\n",
    "\n",
    "        x13 = self.up_conv_2(x12)\n",
    "        x13 = self.dropout(x13)\n",
    "        x14 = torch.cat((x13,self.cropping(x5,x13)), dim=1)\n",
    "\n",
    "        x15 = self.down_conv_7(x14)\n",
    "\n",
    "        x16 = self.up_conv_3(x15)\n",
    "        x16 = self.dropout(x16)\n",
    "        x17 = torch.cat((x16,self.cropping(x3,x16)), dim=1)\n",
    "\n",
    "        x18 = self.down_conv_8(x17)\n",
    "\n",
    "        x19 = self.up_conv_4(x18)\n",
    "        x19 = self.dropout(x19)\n",
    "        x20 = torch.cat((x19,self.cropping(x1,x19)), dim=1)\n",
    "\n",
    "        x21 = self.down_conv_9(x20)\n",
    "\n",
    "        x22 = self.down_conv_final(x21)\n",
    "        return x22\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1d4cdb-8075-41e0-9d1f-5bb59d915372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.rand((2,1,572,572), dtype=torch.float32, requires_grad=True, device='cuda')\n",
    "# m = UNet()\n",
    "# m.to('cuda')\n",
    "# out = m(inp)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5080933-d085-4b4f-9416-6e9eaaab1e79",
   "metadata": {},
   "source": [
    "#### ImagesOnly architecture class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3f3305-a1c9-45ae-8b3c-0fee952f81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesOnly(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # image pathway (Dense UNet)\n",
    "        self.u_net = UNet()\n",
    "        self.u_net = self.u_net.to(device)\n",
    "        \n",
    "        \n",
    "        # combined pathway\n",
    "        # based on (c,h,w), output unet is (2,388,388) and output landmarks ffn is (1,1,64)  \n",
    "        u_net_output = 2*388*388\n",
    "        self.combined_fc = nn.Linear(u_net_output, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.fc4 = nn.Linear(8, 3)\n",
    "\n",
    "        \n",
    "        # dropout feed forward for regularization\n",
    "        self.dropout = nn.Dropout(0.45)  # Dropout layer with dropout rate of 0.4\n",
    "\n",
    "        # activation fn\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # batchnorm\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.bn3 = nn.BatchNorm1d(8)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # unet process\n",
    "        image_features = self.u_net(image)\n",
    "\n",
    "        # flatten both tensor before concatenating, exclude batch_size in BCHW, only flatten CHW\n",
    "        flatten_img = torch.flatten(image_features, start_dim=1) \n",
    "\n",
    "        \n",
    "        x = self.combined_fc(flatten_img)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef239b3a-672c-4ef4-b744-30cbf645d6e2",
   "metadata": {},
   "source": [
    "#### Define cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbfeb7f-3d9b-4b4e-8532-30f7a8495f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcbc48-e23f-48ad-bddf-61348c985424",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import training and test dataset from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4dd9cf3-3a71-48ca-840e-bb345f975321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3ba7f04-3da6-4bcf-96a0-25a4b897bab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PT</th>\n",
       "      <th>MT</th>\n",
       "      <th>TL/L</th>\n",
       "      <th>image_training_file</th>\n",
       "      <th>cobb_angle_training_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.2069</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.261400</td>\n",
       "      <td>sunhl-1th-02-Jan-2017-162 A AP.jpg</td>\n",
       "      <td>sunhl-1th-02-Jan-2017-162 A AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.8107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.851886</td>\n",
       "      <td>sunhl-1th-02-Jan-2017-162 B AP.jpg</td>\n",
       "      <td>sunhl-1th-02-Jan-2017-162 B AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.1172</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sunhl-1th-03-Jan-2017-163 A AP.jpg</td>\n",
       "      <td>sunhl-1th-03-Jan-2017-163 A AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.8144</td>\n",
       "      <td>8.60388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sunhl-1th-03-Jan-2017-163 B AP.jpg</td>\n",
       "      <td>sunhl-1th-03-Jan-2017-163 B AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.1538</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.748480</td>\n",
       "      <td>sunhl-1th-03-Jan-2017-164 A AP.jpg</td>\n",
       "      <td>sunhl-1th-03-Jan-2017-164 A AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>12.5325</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11.484000</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-159 A AP2.jpg</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-159 A AP2.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>11.0118</td>\n",
       "      <td>10.37890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-159 B AP.jpg</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-159 B AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>5.6424</td>\n",
       "      <td>3.95370</td>\n",
       "      <td>2.821400</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-159 C AP.jpg</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-159 C AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>16.3437</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-160 A AP.jpg</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-160 A AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>13.6437</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.947430</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-161 A AP.jpg</td>\n",
       "      <td>sunhl-1th-30-Dec-2016-161 A AP.jpg.mat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PT        MT       TL/L                  image_training_file  \\\n",
       "0     6.2069   0.00000   1.261400   sunhl-1th-02-Jan-2017-162 A AP.jpg   \n",
       "1    23.8107   0.00000   0.851886   sunhl-1th-02-Jan-2017-162 B AP.jpg   \n",
       "2    21.1172   0.00000   0.000000   sunhl-1th-03-Jan-2017-163 A AP.jpg   \n",
       "3    10.8144   8.60388   0.000000   sunhl-1th-03-Jan-2017-163 B AP.jpg   \n",
       "4    18.1538   0.00000   1.748480   sunhl-1th-03-Jan-2017-164 A AP.jpg   \n",
       "..       ...       ...        ...                                  ...   \n",
       "476  12.5325   0.00000  11.484000  sunhl-1th-30-Dec-2016-159 A AP2.jpg   \n",
       "477  11.0118  10.37890   0.000000   sunhl-1th-30-Dec-2016-159 B AP.jpg   \n",
       "478   5.6424   3.95370   2.821400   sunhl-1th-30-Dec-2016-159 C AP.jpg   \n",
       "479  16.3437   0.00000   0.000000   sunhl-1th-30-Dec-2016-160 A AP.jpg   \n",
       "480  13.6437   0.00000   6.947430   sunhl-1th-30-Dec-2016-161 A AP.jpg   \n",
       "\n",
       "                    cobb_angle_training_file  \n",
       "0     sunhl-1th-02-Jan-2017-162 A AP.jpg.mat  \n",
       "1     sunhl-1th-02-Jan-2017-162 B AP.jpg.mat  \n",
       "2     sunhl-1th-03-Jan-2017-163 A AP.jpg.mat  \n",
       "3     sunhl-1th-03-Jan-2017-163 B AP.jpg.mat  \n",
       "4     sunhl-1th-03-Jan-2017-164 A AP.jpg.mat  \n",
       "..                                       ...  \n",
       "476  sunhl-1th-30-Dec-2016-159 A AP2.jpg.mat  \n",
       "477   sunhl-1th-30-Dec-2016-159 B AP.jpg.mat  \n",
       "478   sunhl-1th-30-Dec-2016-159 C AP.jpg.mat  \n",
       "479   sunhl-1th-30-Dec-2016-160 A AP.jpg.mat  \n",
       "480   sunhl-1th-30-Dec-2016-161 A AP.jpg.mat  \n",
       "\n",
       "[481 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training data\n",
    "dataset_training_csv_path = 'C:\\\\Users\\\\iqmal_pc\\\\Desktop\\\\fyp_experiment\\\\training_dataset\\\\angles_ap_labelled_training.csv'\n",
    "dataset_training_csv = pd.read_csv(dataset_training_csv_path)\n",
    "dataset_training_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f261624-3419-479f-aaba-c0708ab283bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PT</th>\n",
       "      <th>MT</th>\n",
       "      <th>TL/L</th>\n",
       "      <th>image_test_file</th>\n",
       "      <th>cobb_angle_test_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.5578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.938543</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-310 C AP.jpg</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-310 C AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.4942</td>\n",
       "      <td>1.00940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-310 a ap.jpg</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-310 a ap.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0048</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.501069</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-311 A AP.jpg</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-311 A AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.3802</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.422200</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-311 C AP.jpg</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-311 C AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1798</td>\n",
       "      <td>1.23238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-311 D AP.jpg</td>\n",
       "      <td>sunhl-1th-01-Mar-2017-311 D AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>7.8933</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.726000</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-307 B AP.jpg</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-307 B AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>22.5108</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>14.726900</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-308 A AP.jpg</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-308 A AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>9.3330</td>\n",
       "      <td>0.73155</td>\n",
       "      <td>6.376700</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-309 A AP.jpg</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-309 A AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>17.5526</td>\n",
       "      <td>8.90332</td>\n",
       "      <td>1.856840</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-309 B AP.jpg</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-309 B AP.jpg.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>14.7518</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.070970</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-310 B AP.jpg</td>\n",
       "      <td>sunhl-1th-28-Feb-2017-310 B AP.jpg.mat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PT       MT       TL/L                     image_test_file  \\\n",
       "0    12.5578  0.00000   0.938543  sunhl-1th-01-Mar-2017-310 C AP.jpg   \n",
       "1     7.4942  1.00940   0.000000  sunhl-1th-01-Mar-2017-310 a ap.jpg   \n",
       "2    14.0048  0.00000   0.501069  sunhl-1th-01-Mar-2017-311 A AP.jpg   \n",
       "3    13.3802  0.00000   1.422200  sunhl-1th-01-Mar-2017-311 C AP.jpg   \n",
       "4    16.1798  1.23238   0.000000  sunhl-1th-01-Mar-2017-311 D AP.jpg   \n",
       "..       ...      ...        ...                                 ...   \n",
       "123   7.8933  0.00000   1.726000  sunhl-1th-28-Feb-2017-307 B AP.jpg   \n",
       "124  22.5108  0.00000  14.726900  sunhl-1th-28-Feb-2017-308 A AP.jpg   \n",
       "125   9.3330  0.73155   6.376700  sunhl-1th-28-Feb-2017-309 A AP.jpg   \n",
       "126  17.5526  8.90332   1.856840  sunhl-1th-28-Feb-2017-309 B AP.jpg   \n",
       "127  14.7518  0.00000   5.070970  sunhl-1th-28-Feb-2017-310 B AP.jpg   \n",
       "\n",
       "                       cobb_angle_test_file  \n",
       "0    sunhl-1th-01-Mar-2017-310 C AP.jpg.mat  \n",
       "1    sunhl-1th-01-Mar-2017-310 a ap.jpg.mat  \n",
       "2    sunhl-1th-01-Mar-2017-311 A AP.jpg.mat  \n",
       "3    sunhl-1th-01-Mar-2017-311 C AP.jpg.mat  \n",
       "4    sunhl-1th-01-Mar-2017-311 D AP.jpg.mat  \n",
       "..                                      ...  \n",
       "123  sunhl-1th-28-Feb-2017-307 B AP.jpg.mat  \n",
       "124  sunhl-1th-28-Feb-2017-308 A AP.jpg.mat  \n",
       "125  sunhl-1th-28-Feb-2017-309 A AP.jpg.mat  \n",
       "126  sunhl-1th-28-Feb-2017-309 B AP.jpg.mat  \n",
       "127  sunhl-1th-28-Feb-2017-310 B AP.jpg.mat  \n",
       "\n",
       "[128 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "dataset_test_csv_path = 'C:\\\\Users\\\\iqmal_pc\\\\Desktop\\\\fyp_experiment\\\\test_dataset\\\\angles_ap_labelled_test.csv'\n",
    "dataset_test_csv = pd.read_csv(dataset_test_csv_path)\n",
    "dataset_test_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16fa58-c0b5-4b14-8694-86eb12213c80",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare dictionary for training image data and its Cobb angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820235bb-0300-4356-a45f-9684c6571bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_dict = {} # good approach to maintain image - target_angle relation\n",
    "\n",
    "# get train image list and its path\n",
    "training_image_path = 'C:\\\\Users\\\\iqmal_pc\\\\Desktop\\\\fyp_experiment\\\\data\\\\training\\\\'\n",
    "\n",
    "# get list of keys ie filepath\n",
    "training_image_filepath_key = dataset_training_csv['image_training_file'].values.tolist()\n",
    "\n",
    "# get list of values for dict\n",
    "training_cobb_angle_values = dataset_training_csv[['PT', 'MT', 'TL/L']].values\n",
    "\n",
    "# add to dictionary\n",
    "for i in range(len(training_image_filepath_key)):\n",
    "    training_dataset_dict[training_image_path + training_image_filepath_key[i]] = torch.tensor(training_cobb_angle_values[i], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f80da-0d9b-4043-a03e-4bdfd7748059",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare dictionary for test image data and its Cobb angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d19774-7db5-400f-acd3-49b3431896bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dict = {}\n",
    "test_image_path = 'C:\\\\Users\\\\iqmal_pc\\\\Desktop\\\\fyp_experiment\\\\data\\\\test\\\\'\n",
    "test_image_filepath_key = dataset_test_csv['image_test_file'].values.tolist()\n",
    "test_cobb_angle_values = dataset_test_csv[['PT', 'MT', 'TL/L']].values\n",
    "\n",
    "for i in range(len(test_image_filepath_key)):\n",
    "    test_dataset_dict[test_image_path + test_image_filepath_key[i]] = torch.tensor(test_cobb_angle_values[i], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48494043-079c-4958-a68d-bc11cc9865cd",
   "metadata": {},
   "source": [
    "#### CustomDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78af99df-a798-4777-9e79-cbe2208dc23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageOps\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d703c34e-e5d0-43cd-81c7-d703b4834766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary dataset where key is image path and value is PT, MT, TL/L values\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, cobb_angle_list, transform_image_tensor=None, custom_img_resize_w_h=(100,100)):\n",
    "        self.images = image_list\n",
    "        # convert python list of torch.tensor to torch tensor\n",
    "        self.cobb_angles = torch.stack(cobb_angle_list, dim=0) \n",
    "        self.transform_image_tensor = transform_image_tensor\n",
    "        self.custom_img_resize_w_h = custom_img_resize_w_h\n",
    "        self.mean_img = 0.0\n",
    "        self.std_img = 0.0\n",
    "        self.mean_cobb_angle = 0.0\n",
    "        self.std_cobb_angle = 0.0\n",
    "\n",
    "        \n",
    "        self.calculate_mean_std_img()\n",
    "        self.calculate_mean_std_cobb_angle()\n",
    "\n",
    "        # normalize dataset once\n",
    "        self.cobb_angles = (self.cobb_angles - self.mean_cobb_angle)/self.std_cobb_angle\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index] # image_path is the key\n",
    "        \n",
    "        image = Image.open(image_path).convert('L')\n",
    "        \n",
    "        # custom resize image while maintaining aspect ratio\n",
    "        image = self.resize_with_padding(image, self.custom_img_resize_w_h) # operation in PIL\n",
    "        image = np.asarray(image) # dimension HW after numpy\n",
    "        # print('before adding channel dim ', image.shape)\n",
    "        image = np.expand_dims(image, axis=0) # now it is CHW\n",
    "        # print('after expand for channel dim ', image.shape)\n",
    "        \n",
    "\n",
    "        \n",
    "        # calculate mean and std of entire images\n",
    "        # z_norm_img = transforms.Normalize(self.mean_img, self.std_img)\n",
    "        \n",
    "        # normalize image\n",
    "        image_z_norm = (image - self.mean_img) / self.std_img\n",
    "        \n",
    "        # convert to tensor\n",
    "        image_normalized = torch.tensor(image_z_norm, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        \n",
    "        # transform image if required\n",
    "        # if self.transform_image_tensor is not None:\n",
    "            # image_normalized = self.transform_image_tensor(image_z_norm) # onvert to tensor\n",
    "            # image = torch.permute(image, dims=(0,2,1)) # convert PIL (w,h) to tensor (h,w)\n",
    "            # image = z_norm_img(image)\n",
    "            \n",
    "        # transform ie normalize cobb_angle\n",
    "        # z_norm_cobb_angle = transforms.Normalize(self.mean_cobb_angle, self.std_cobb_angle)\n",
    "        # self.cobb_angles = z_norm_cobb_angle(self.cobb_angles)\n",
    "        # self.cobb_angles = (self.cobb_angles - self.mean_cobb_angle)/self.std_cobb_angle\n",
    "        \n",
    "        # transform ie normalize landmark\n",
    "        # z_norm_landmark = transforms.Normalize(self.mean_landmark, self.std_landmark)\n",
    "        # self.landmark = z_norm_landmark(self.landmarks)\n",
    "        # self.landmarks = (self.landmarks - self.mean_landmark)/self.std_landmark\n",
    "        \n",
    "\n",
    "        # because U-net is hardcoded for 3 channel, duplicate another 2 channel\n",
    "        # image = torch.cat((image[:,:,:], image[:,:,:], image[:,:,:]), dim=0)\n",
    "        # print('tensor image shape ', image_normalized.shape)\n",
    "        return image_normalized, self.cobb_angles[index], image_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def resize_with_padding(self, image, output_size):\n",
    "        original_width, original_height = image.size\n",
    "        target_width, target_height = output_size\n",
    "\n",
    "        # Calculate aspect ratios\n",
    "        aspect_ratio = original_width / original_height\n",
    "        target_ratio = target_width / target_height\n",
    "\n",
    "        # Calculate the new dimensions while maintaining aspect ratio\n",
    "        if aspect_ratio > target_ratio:\n",
    "            new_width = target_width\n",
    "            new_height = int(new_width / aspect_ratio)\n",
    "        else:\n",
    "            new_height = target_height\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "        # Resize the image while preserving the aspect ratio\n",
    "        resized_image = image.resize((new_width, new_height), resample=Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Create a new image with the target size and paste the resized image onto it\n",
    "        padded_image = Image.new('L', (target_width, target_height))\n",
    "        x_offset = (target_width - new_width) // 2\n",
    "        y_offset = (target_height - new_height) // 2\n",
    "        padded_image.paste(resized_image, (x_offset, y_offset))\n",
    "\n",
    "        return padded_image\n",
    "    \n",
    "    def calculate_mean_std_img(self):\n",
    "        num_images = len(self.images)\n",
    "        mean_list = np.zeros((num_images,))\n",
    "        std_list = np.zeros((num_images,))\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            image_path = self.images[i]\n",
    "            image = Image.open(image_path).convert('L')\n",
    "            image = self.resize_with_padding(image, self.custom_img_resize_w_h)\n",
    "            image = np.asarray(image) # HW after numpy\n",
    "            # add number of channel 1 as grayscale dimension often ignored\n",
    "            image = np.expand_dims(image, axis=0) # now it is CHW\n",
    "            \n",
    "            mean_list[i] = np.mean(image)\n",
    "            std_list[i] = np.std(image)\n",
    "            \n",
    "#             image_tensor = ToTensor()(image)\n",
    "#             image_tensor = torch.permute(image_tensor, dims=(0,2,1)) # convert PIL (w,h) to tensor (h,w)\n",
    "            \n",
    "#             mean_list[i] = torch.mean(image_tensor)\n",
    "#             std_list[i] = torch.std(image_tensor)\n",
    "\n",
    "        # calculate combined std\n",
    "        var_list = np.array([element**2 for element in std_list])\n",
    "        combined_std = np.sqrt(np.sum(var_list)/std_list.shape[0])\n",
    "        self.std_img = combined_std # result output\n",
    "        \n",
    "        # calculate combined mean\n",
    "        combined_mean = np.mean(mean_list)\n",
    "        self.mean_img = combined_mean # result output\n",
    "        \n",
    "    def calculate_mean_std_cobb_angle(self):\n",
    "        num_cobb_angle = self.cobb_angles.shape[0]\n",
    "        PT_values_list = np.zeros((num_cobb_angle,))\n",
    "        MT_values_list = np.zeros((num_cobb_angle,))\n",
    "        TL_values_list = np.zeros((num_cobb_angle,))\n",
    "        \n",
    "        \n",
    "        for i in range(num_cobb_angle):\n",
    "            PT_values_list[i] = self.cobb_angles[i][0]\n",
    "            MT_values_list[i] = self.cobb_angles[i][1]\n",
    "            TL_values_list[i] = self.cobb_angles[i][2]\n",
    "            \n",
    "        \n",
    "        PT_mean = np.mean(PT_values_list)\n",
    "        MT_mean = np.mean(MT_values_list)\n",
    "        TL_mean = np.mean(TL_values_list)\n",
    "        \n",
    "        PT_std = np.std(PT_values_list)\n",
    "        MT_std = np.std(MT_values_list)\n",
    "        TL_std = np.std(TL_values_list)\n",
    "        \n",
    "        #calculate combined std\n",
    "        var_list = np.array([PT_std**2, MT_std**2, TL_std**2])\n",
    "        combined_std = np.sqrt(np.sum(var_list)/3)\n",
    "        self.std_cobb_angle = combined_std\n",
    "        \n",
    "        # calculate combined mean\n",
    "        combined_mean = np.mean([PT_mean,MT_mean,TL_mean])\n",
    "        self.mean_cobb_angle = combined_mean\n",
    "        \n",
    "        print('std_cobb_angle ', combined_std)\n",
    "        print('mean_cobb_angle ', combined_mean)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819842b-18e5-455b-b017-9d1fbdf8caf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tensor Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a50fc123-7741-4c8c-aa78-fc7a5f1c8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_images_tensor = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Lambda(lambda x: x.type(torch.float32))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fe53a-0005-4003-952b-928804dc802b",
   "metadata": {},
   "source": [
    "#### Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f235b38-a4f5-4f95-aabb-937460cc0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c0d2ee8-9df5-4bd7-ad43-dbc7bda5299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to list before inputting into splitting function\n",
    "training_image_list = list(training_dataset_dict.keys())\n",
    "training_true_cobb_angle_list = list(training_dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce7afd27-57ec-4464-9a1a-0864f06058a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lists into training and validation sets\n",
    "training_image, val_image, training_true_cobb_angle, val_true_cobb_angle = train_test_split(training_image_list, training_true_cobb_angle_list, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166943a5-81fb-4477-b5da-a3653e11cd73",
   "metadata": {},
   "source": [
    "#### Custom Dataset instance for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36a19185-4866-4fe3-9875-349118ca92f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed_training_dataset\n",
      "std_cobb_angle  3.2215509395355943\n",
      "mean_cobb_angle  5.493506302859815\n",
      "transformed_validation_dataset\n",
      "std_cobb_angle  3.4379941274198913\n",
      "mean_cobb_angle  5.729382691079194\n"
     ]
    }
   ],
   "source": [
    "print('transformed_training_dataset')\n",
    "transformed_training_dataset = CustomDataset(training_image, training_true_cobb_angle, custom_img_resize_w_h=(572,572))\n",
    "print('transformed_validation_dataset')\n",
    "transformed_validation_dataset = CustomDataset(val_image, val_true_cobb_angle, custom_img_resize_w_h=(572,572))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a6c0501-1512-49ce-8a01-940c15296096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, landmark, cobb, path = transformed_training_dataset.__getitem__(2)\n",
    "# print('cobb ', cobb)\n",
    "# print('imgfile ', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ffdef5-c586-4e4a-8fb4-39654591457c",
   "metadata": {},
   "source": [
    "#### training_loader and validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a06d30f3-0283-4f8c-810d-86818e2a5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e1f2dd9-d8d1-4c24-ae6c-b8556527aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader for batching and shuffling the data\n",
    "# batch_size = 32\n",
    "\n",
    "training_loader = DataLoader(transformed_training_dataset, batch_size=2, shuffle=True)\n",
    "validation_loader =  DataLoader(transformed_validation_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "233d4f17-f6b0-47e8-b2f4-8b026ce097bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scractcpad\n",
    "# model_try = MultiInputNet(landmark_input_size=64, landmark_output_size=64)\n",
    "# model_try.to(device)\n",
    "# for img, land, target, imagepath in training_loader:\n",
    "#     img = img.to(device)\n",
    "#     land = land.to(device)\n",
    "#     target = target.to(device)\n",
    "#     print('imagepath ', imagepath)\n",
    "#     print('target angle ', target)\n",
    "#     predicted = model_try(img, land)\n",
    "#     print('predicted ', predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff036a-0a12-4bec-bcbf-9cbd5f46aa21",
   "metadata": {},
   "source": [
    "#### Training and validating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54e13c3e-0b31-4a7f-b2b5-ca46125515c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63d781ce-2cb1-45a5-a28b-868c22345f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImagesOnly(\n",
      "  (u_net): UNet(\n",
      "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "    (down_conv_1): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (down_conv_2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (down_conv_3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (down_conv_4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (down_conv_5): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (up_conv_1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (up_conv_2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (up_conv_3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (up_conv_4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (down_conv_6): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (down_conv_7): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (down_conv_8): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (down_conv_9): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (down_conv_final): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (combined_fc): Linear(in_features=301088, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.45, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ImagesOnly()\n",
    "print(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a02d54bc-9bd4-42dd-aaf9-a9e3380ee039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (image, *_) in enumerate(training_loader):\n",
    "#     print(str(idx) + ' -> ' + str(image.shape) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f11c67ca-5bc1-4114-a24f-f74088d636b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "# set up optimizer and loss function\n",
    "base_lr = 1e-3\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=base_lr, weight_decay=1e-3 / 200)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=24, T_mult=1, eta_min=1e-4, last_epoch=- 1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6323c59f-944c-4d42-b3ee-dfd54cead1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epochs and the patience for early stopping\n",
    "num_epochs = int(1e6)\n",
    "patience = 10\n",
    "best_model_state_dict = None\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# path for saving model\n",
    "path = 'C:\\\\Users\\\\iqmal_pc\\\\Desktop\\\\fyp_experiment\\\\saved_models\\\\training_11\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea2a2d17-08de-4e9a-8e81-b9c1557d437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000000], Training Loss: 2.9580975444987416, Validation Loss: 2.5904974500389444\n",
      "Epoch 00001: adjusting learning rate of group 0 to 9.9615e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [2/1000000], Training Loss: 2.0830534851799407, Validation Loss: 2.3566472788062907\n",
      "Epoch 00002: adjusting learning rate of group 0 to 9.8467e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [3/1000000], Training Loss: 1.7495081405310582, Validation Loss: 1.8402957558324657\n",
      "Epoch 00003: adjusting learning rate of group 0 to 9.6575e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [4/1000000], Training Loss: 1.6742809417967994, Validation Loss: 1.785070070286387\n",
      "Epoch 00004: adjusting learning rate of group 0 to 9.3971e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [5/1000000], Training Loss: 1.5585183443812032, Validation Loss: 1.774813399486935\n",
      "Epoch 00005: adjusting learning rate of group 0 to 9.0701e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [6/1000000], Training Loss: 1.5650495418036978, Validation Loss: 1.4518179048582451\n",
      "Epoch 00006: adjusting learning rate of group 0 to 8.6820e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [7/1000000], Training Loss: 1.3832949184191723, Validation Loss: 3079.552040159088\n",
      "Epoch 00007: adjusting learning rate of group 0 to 8.2394e-04.\n",
      "Epoch [8/1000000], Training Loss: 1.3806711327439796, Validation Loss: 1.4364738342995496\n",
      "Epoch 00008: adjusting learning rate of group 0 to 7.7500e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [9/1000000], Training Loss: 1.3658600749913603, Validation Loss: 1.2488409082990946\n",
      "Epoch 00009: adjusting learning rate of group 0 to 7.2221e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [10/1000000], Training Loss: 1.2936588750065614, Validation Loss: 1.2637777509763068\n",
      "Epoch 00010: adjusting learning rate of group 0 to 6.6647e-04.\n",
      "Epoch [11/1000000], Training Loss: 1.3167787610242765, Validation Loss: 1.4738626633722758\n",
      "Epoch 00011: adjusting learning rate of group 0 to 6.0874e-04.\n",
      "Epoch [12/1000000], Training Loss: 1.205183362821117, Validation Loss: 1.4159714384484536\n",
      "Epoch 00012: adjusting learning rate of group 0 to 5.5000e-04.\n",
      "Epoch [13/1000000], Training Loss: 1.2341745117058356, Validation Loss: 1.4374467722073043\n",
      "Epoch 00013: adjusting learning rate of group 0 to 4.9126e-04.\n",
      "Epoch [14/1000000], Training Loss: 1.2735007739004989, Validation Loss: 1.2475250202816786\n",
      "Epoch 00014: adjusting learning rate of group 0 to 4.3353e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [15/1000000], Training Loss: 1.17623347917106, Validation Loss: 1.2202800490807013\n",
      "Epoch 00015: adjusting learning rate of group 0 to 3.7779e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [16/1000000], Training Loss: 1.2204386591135215, Validation Loss: 1.2097961335759801\n",
      "Epoch 00016: adjusting learning rate of group 0 to 3.2500e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [17/1000000], Training Loss: 1.1513871094988037, Validation Loss: 1.1432277111663032\n",
      "Epoch 00017: adjusting learning rate of group 0 to 2.7606e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [18/1000000], Training Loss: 1.15592510435575, Validation Loss: 1.2358002547443527\n",
      "Epoch 00018: adjusting learning rate of group 0 to 2.3180e-04.\n",
      "Epoch [19/1000000], Training Loss: 1.1381483364384621, Validation Loss: 1.31800020900891\n",
      "Epoch 00019: adjusting learning rate of group 0 to 1.9299e-04.\n",
      "Epoch [20/1000000], Training Loss: 1.1772585158081104, Validation Loss: 1.3104229578652333\n",
      "Epoch 00020: adjusting learning rate of group 0 to 1.6029e-04.\n",
      "Epoch [21/1000000], Training Loss: 1.1855856752954423, Validation Loss: 1.2523866785863011\n",
      "Epoch 00021: adjusting learning rate of group 0 to 1.3425e-04.\n",
      "Epoch [22/1000000], Training Loss: 1.2080040608998388, Validation Loss: 1.2838003710680401\n",
      "Epoch 00022: adjusting learning rate of group 0 to 1.1533e-04.\n",
      "Epoch [23/1000000], Training Loss: 1.1925665326416492, Validation Loss: 1.320171215024191\n",
      "Epoch 00023: adjusting learning rate of group 0 to 1.0385e-04.\n",
      "Epoch [24/1000000], Training Loss: 1.1740646217949688, Validation Loss: 1.2382564227451984\n",
      "Epoch 00024: adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch [25/1000000], Training Loss: 1.1532521444993715, Validation Loss: 1.130576981481203\n",
      "Epoch 00025: adjusting learning rate of group 0 to 9.9615e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [26/1000000], Training Loss: 1.1171556181119133, Validation Loss: 1.1508461588359986\n",
      "Epoch 00026: adjusting learning rate of group 0 to 9.8467e-04.\n",
      "Epoch [27/1000000], Training Loss: 1.1042117522641395, Validation Loss: 1.151095243305275\n",
      "Epoch 00027: adjusting learning rate of group 0 to 9.6575e-04.\n",
      "Epoch [28/1000000], Training Loss: 1.058856677962467, Validation Loss: 1.1460272988554128\n",
      "Epoch 00028: adjusting learning rate of group 0 to 9.3971e-04.\n",
      "Epoch [29/1000000], Training Loss: 1.1450932622731973, Validation Loss: 1.118058083463729\n",
      "Epoch 00029: adjusting learning rate of group 0 to 9.0701e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [30/1000000], Training Loss: 1.0960934844139654, Validation Loss: 1.1402469512420832\n",
      "Epoch 00030: adjusting learning rate of group 0 to 8.6820e-04.\n",
      "Epoch [31/1000000], Training Loss: 1.072536350809969, Validation Loss: 1.116931097745204\n",
      "Epoch 00031: adjusting learning rate of group 0 to 8.2394e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [32/1000000], Training Loss: 1.100376480491832, Validation Loss: 1.0739157220358317\n",
      "Epoch 00032: adjusting learning rate of group 0 to 7.7500e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [33/1000000], Training Loss: 1.086751381905439, Validation Loss: 1.1103647568637562\n",
      "Epoch 00033: adjusting learning rate of group 0 to 7.2221e-04.\n",
      "Epoch [34/1000000], Training Loss: 1.074602748422573, Validation Loss: 1.1104039186707784\n",
      "Epoch 00034: adjusting learning rate of group 0 to 6.6647e-04.\n",
      "Epoch [35/1000000], Training Loss: 1.0682925836105521, Validation Loss: 1.0896013827091946\n",
      "Epoch 00035: adjusting learning rate of group 0 to 6.0874e-04.\n",
      "Epoch [36/1000000], Training Loss: 1.0885007425677031, Validation Loss: 1.0816348560980147\n",
      "Epoch 00036: adjusting learning rate of group 0 to 5.5000e-04.\n",
      "Epoch [37/1000000], Training Loss: 1.0451498281133051, Validation Loss: 1.1014502800847452\n",
      "Epoch 00037: adjusting learning rate of group 0 to 4.9126e-04.\n",
      "Epoch [38/1000000], Training Loss: 1.055741057653601, Validation Loss: 1.0665759406407778\n",
      "Epoch 00038: adjusting learning rate of group 0 to 4.3353e-04.\n",
      "Lowest Validation Found\n",
      "Epoch [39/1000000], Training Loss: 1.0783898573912059, Validation Loss: 1.0680419161301298\n",
      "Epoch 00039: adjusting learning rate of group 0 to 3.7779e-04.\n",
      "Epoch [40/1000000], Training Loss: 1.0280802513007075, Validation Loss: 1.0834243143092572\n",
      "Epoch 00040: adjusting learning rate of group 0 to 3.2500e-04.\n",
      "Epoch [41/1000000], Training Loss: 1.0323565202609946, Validation Loss: 1.0767771772963484\n",
      "Epoch 00041: adjusting learning rate of group 0 to 2.7606e-04.\n",
      "Epoch [42/1000000], Training Loss: 1.0379329786325495, Validation Loss: 1.0752424858030432\n",
      "Epoch 00042: adjusting learning rate of group 0 to 2.3180e-04.\n",
      "Epoch [43/1000000], Training Loss: 1.0467703453420352, Validation Loss: 1.076420817860239\n",
      "Epoch 00043: adjusting learning rate of group 0 to 1.9299e-04.\n",
      "Epoch [44/1000000], Training Loss: 1.0262797587007906, Validation Loss: 1.0734881115405215\n",
      "Epoch 00044: adjusting learning rate of group 0 to 1.6029e-04.\n",
      "Epoch [45/1000000], Training Loss: 1.0655580354699243, Validation Loss: 1.0737206130528574\n",
      "Epoch 00045: adjusting learning rate of group 0 to 1.3425e-04.\n",
      "Epoch [46/1000000], Training Loss: 1.0114926661287125, Validation Loss: 1.0719584706863485\n",
      "Epoch 00046: adjusting learning rate of group 0 to 1.1533e-04.\n",
      "Epoch [47/1000000], Training Loss: 1.0339826517738402, Validation Loss: 1.0828057620290321\n",
      "Epoch 00047: adjusting learning rate of group 0 to 1.0385e-04.\n",
      "Epoch [48/1000000], Training Loss: 1.044212721773268, Validation Loss: 1.073941463297329\n",
      "Epoch 00048: adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Early stopping at epoch 47\n"
     ]
    }
   ],
   "source": [
    "# to save validation loss and training loss\n",
    "loss_list_data = []\n",
    "lr = []\n",
    "\n",
    "# to keep track lowest loss\n",
    "lowest_val_loss = float('inf')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # training phase\n",
    "    model.train()                                       # allow weights and biases adjusting\n",
    "    training_loss = 0.0\n",
    "    for idx, (image_input, target_angle, _) in enumerate(training_loader):\n",
    "        # move input and output to cuda if available\n",
    "        image_input = image_input.to(device)\n",
    "        target_angle = target_angle.to(device)\n",
    "        optimizer.zero_grad()                           # clearing previous loss derivative\n",
    "        # print('training: ', idx)\n",
    "        predicted = model(image_input)                  # forward \n",
    "        loss = criterion(predicted, target_angle)       # computing loss\n",
    "        loss.backward()                                 # computing loss derivative with respect of weights and biases\n",
    "        optimizer.step()                                # adjusting weights and biases\n",
    "        training_loss += loss.item()                    # summing all batches losses for averaging\n",
    "    training_loss /= len(training_loader)\n",
    "        \n",
    "    # validation phase in at end of every epoch\n",
    "    model.eval()                                        # block weights and biases adjsuting\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():                                # remove all computed derivatives\n",
    "        for idx, (image_input, target_angle, _) in enumerate(validation_loader):\n",
    "            # move input and output to cuda if available\n",
    "            image_input = image_input.to(device)\n",
    "            target_angle = target_angle.to(device)\n",
    "            # print('testing: ', idx)\n",
    "            predicted = model(image_input)\n",
    "            # for averaging validation loss\n",
    "            val_loss += criterion(predicted, target_angle).item()\n",
    "    \n",
    "    val_loss /= len(validation_loader) # calculating average validation loss\n",
    "    \n",
    "    # Print the training and validation loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {training_loss}, Validation Loss: {val_loss}\")    \n",
    "    \n",
    "    # for graphing evaluation later\n",
    "    loss_list_data.append((epoch+1, training_loss, val_loss))\n",
    "    # for graphing learning rate vs epoch later\n",
    "    lr.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if val_loss < lowest_val_loss:\n",
    "        lowest_val_loss = val_loss\n",
    "        best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_state_dict, path + 'best_training_11.pt')\n",
    "        print('Lowest Validation Found')\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d18b4-36ec-4d3b-8996-284046cd9afe",
   "metadata": {},
   "source": [
    "#### Compute training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "733291e7-c3b9-4b03-9da6-26790337e604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 5482.723794698715 seconds\n"
     ]
    }
   ],
   "source": [
    "training_time = end_time - start_time\n",
    "print(\"Training time elapsed: \" + str(training_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ee8a6-b7f7-455f-8bf2-47da08429cbe",
   "metadata": {},
   "source": [
    "#### Save latest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7186967-7aa2-495f-ad3f-578ae1d04cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\iqmal_pc\\\\Desktop\\\\fyp_experiment\\\\saved_models\\\\training_11\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fce3733-3ebb-4b5e-b402-fc63120b3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "torch.save(model.state_dict(), path + 'finished_training_11.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5c6e6-eff7-4397-b749-5019ada70292",
   "metadata": {},
   "source": [
    "#### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05943ef7-55e7-4671-a13a-7ed685d6d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack list\n",
    "test_image = list(test_dataset_dict.keys())\n",
    "test_true_cobb_angle = list(test_dataset_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85b94094-2547-41c6-9f87-6088ec8907d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform_images_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loading test dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m transformed_test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(test_image, test_true_cobb_angle, \u001b[43mtransform_images_tensor\u001b[49m, (\u001b[38;5;241m572\u001b[39m,\u001b[38;5;241m572\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform_images_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "# loading test dataset\n",
    "transformed_test_dataset = CustomDataset(test_image, test_true_cobb_angle, transform_images_tensor, (572,572))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb51b8-7d2a-4796-addd-d5bb4afcf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data loader\n",
    "test_loader =  DataLoader(transformed_test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f9ee0-8e3f-46bb-b6c1-555a6a917b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for image_input, target_angle, _ in test_loader:\n",
    "        # move input and output to cuda if available\n",
    "        image_input = image_input.to(device)\n",
    "        target_angle = target_angle.to(device)\n",
    "        predicted = model(image_input)\n",
    "        test_loss += criterion(predicted, target_angle).item()\n",
    "\n",
    "test_loss /= len(test_loader) # calculating average test_loss\n",
    "print(f\"Final model testing loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197d2c2-d205-42ea-8563-c79c246bbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "the_best_model = ImagesOnly()\n",
    "the_best_model.load_state_dict(torch.load(path + 'best_training_11.pt')) # load the saved best model weights & biases dictionary\n",
    "the_best_model = the_best_model.to(device)\n",
    "# testing phase using best model\n",
    "test_loss = 0.0\n",
    "the_best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for image_input, target_angle, _ in test_loader:\n",
    "        # move input and output to cuda if available\n",
    "        image_input = image_input.to(device)\n",
    "        target_angle = target_angle.to(device)\n",
    "        predicted = the_best_model(image_input)\n",
    "        test_loss += criterion(predicted, target_angle).item()\n",
    "\n",
    "test_loss /= len(test_loader) # calculating average test_loss\n",
    "print(f\"Best model testing loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd681c10-a681-4e40-95fc-fbf62d55d9ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation (Loss vs Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abacd86-a8b5-4942-94a7-f95f7cbb0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list = []\n",
    "training_loss_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "for i in range(len(loss_list_data)):\n",
    "    epoch_list.append(loss_list_data[i][0])\n",
    "    training_loss_list.append(loss_list_data[i][1])\n",
    "    validation_loss_list.append(loss_list_data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2956b-6b84-4a72-9b57-feebd4c61e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss vs epoch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_list, training_loss_list, color='#f70c0c', label='training loss')\n",
    "plt.plot(epoch_list, validation_loss_list, color='#00fa08', label='validation loss')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354abcab-855b-445f-87cb-58851248f0c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### learning rate vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533659a7-9be0-4c6e-b87f-b58f8ab1abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss vs epoch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_list, lr, color='#f70c0c')\n",
    "plt.title('learning rate vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8571a9d-179d-4b27-b1b3-7d3419f32a62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test data evaluation plot (Predicted vs Ground Truth) for every PT, MT and TL/L using final_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25dcc0a-31a8-4626-9411-d4fddd3485bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_trained_model = ImagesOnly()\n",
    "final_trained_model.load_state_dict(torch.load(path + 'finished_training_11.pt')) # load the saved best model weights & biases dictionary\n",
    "final_trained_model = final_trained_model.to(device)\n",
    "\n",
    "predicted_list_final = []\n",
    "target_angle_list_final = []\n",
    "\n",
    "final_trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    for image_input, target_angle, _ in test_loader:\n",
    "        # move input and output to cuda if available\n",
    "        image_input = image_input.to(device)\n",
    "        target_angle = target_angle.to(device)\n",
    "        predicted = the_best_model(image_input)\n",
    "        predicted_list_final.append(predicted.cpu().numpy())\n",
    "        target_angle_list_final.append(target_angle.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f53486b-0397-4554-a82b-77952e7a7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_cobb_angle_test = 2.988889043938208\n",
    "mean_cobb_angle_test = 5.054406259208918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91fc20-502d-4b47-9b1c-2824b212eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping from (1,3) to (3,)\n",
    "for i in range(len(predicted_list_final)):\n",
    "    predicted_list_final[i] = predicted_list_final[i].reshape((3,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ad3e2-d038-4a0e-bdbf-418c572e5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping from (1,3) to (3,)\n",
    "for i in range(len(target_angle_list_final)):\n",
    "    target_angle_list_final[i] = target_angle_list_final[i].reshape((3,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d458e2d-92e0-463a-ab86-ebb602b316e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined PT (predicted, true)\n",
    "PT_value_final = [(PT_predicted, PT_true) for (PT_predicted,*_),(PT_true,*_) in zip(predicted_list_final,target_angle_list_final)]\n",
    "\n",
    "# Convert x-score value to original value\n",
    "for i in range(len(PT_value_final)):\n",
    "    temp = list(PT_value_final[i])\n",
    "    temp[0] = std_cobb_angle_test*temp[0] + mean_cobb_angle_test\n",
    "    temp[1] = std_cobb_angle_test*temp[1] + mean_cobb_angle_test\n",
    "    PT_value_final[i] = tuple(temp)\n",
    "\n",
    "# PT_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25fa8b-beef-4f6e-8f74-bf4b57e944f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT_value_final = [(MT_predicted,MT_true) for (_,MT_predicted,_),(_,MT_true,_) in zip(predicted_list_final,target_angle_list_final)]\n",
    "\n",
    "# Convert x-score value to original value\n",
    "for i in range(len(MT_value_final)):\n",
    "    temp = list(MT_value_final[i])\n",
    "    temp[0] = std_cobb_angle_test*temp[0] + mean_cobb_angle_test\n",
    "    temp[1] = std_cobb_angle_test*temp[1] + mean_cobb_angle_test\n",
    "    MT_value_final[i] = tuple(temp)\n",
    "\n",
    "# MT_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47298d6e-9e4d-469a-8c9a-2b38725ba996",
   "metadata": {},
   "outputs": [],
   "source": [
    "TL_value_final = [(TL_predicted,TL_true) for (*_,TL_predicted),(*_,TL_true) in zip(predicted_list_final,target_angle_list_final)]\n",
    "\n",
    "# Convert x-score value to original value\n",
    "for i in range(len(TL_value_final)):\n",
    "    temp = list(TL_value_final[i])\n",
    "    temp[0] = std_cobb_angle_test*temp[0] + mean_cobb_angle_test\n",
    "    temp[1] = std_cobb_angle_test*temp[1] + mean_cobb_angle_test\n",
    "    TL_value_final[i] = tuple(temp)\n",
    "\n",
    "# TL_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec25ce5-ecf4-48e2-9459-6e95537028e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PT value\n",
    "\n",
    "predicted_pt = [item[0] for item in PT_value_final]\n",
    "true_pt = [item[1] for item in PT_value_final]\n",
    "\n",
    "plt.scatter(true_pt, predicted_pt, color='#fc0505', s=5)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('PT angle (final epoch model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a5d3b-9776-4d4c-aa93-0fc35aed8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot MT value\n",
    "\n",
    "predicted_mt = [item[0] for item in MT_value_final]\n",
    "true_mt = [item[1] for item in MT_value_final]\n",
    "\n",
    "plt.scatter(true_mt, predicted_mt, color='#fc0505', s=5)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('MT angle (final epoch model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904df6c6-fdd0-46a6-ac30-72614ee41b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot TL value\n",
    "\n",
    "predicted_tl = [item[0] for item in TL_value_final]\n",
    "true_tl = [item[1] for item in TL_value_final]\n",
    "\n",
    "plt.scatter(true_tl, predicted_tl, color='#fc0505', s=5)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('TL angle (final epoch model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13744d3f-6f58-4f1f-8eef-561cdf124deb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test data evaluation plot (Predicted vs Ground Truth) for every PT, MT and TL/L using the_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183ad17-f94b-40eb-8b7e-a18a72aedbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list_best = []\n",
    "target_angle_list_best = []\n",
    "\n",
    "the_best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for image_input, target_angle, _ in test_loader:\n",
    "        # move input and output to cuda if available\n",
    "        image_input = image_input.to(device)\n",
    "        target_angle = target_angle.to(device)\n",
    "        predicted = the_best_model(image_input)\n",
    "        predicted_list_best.append(predicted.cpu().numpy())\n",
    "        target_angle_list_best.append(target_angle.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930c0e5-dfa8-4623-bc0a-d2a3a71fcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_cobb_angle_test = 2.988889043938208\n",
    "mean_cobb_angle_test = 5.054406259208918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332cf411-870a-43cf-b8b1-37aeeda0ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping from (1,3) to (3,)\n",
    "for i in range(len(predicted_list_best)):\n",
    "    predicted_list_best[i] = predicted_list_best[i].reshape((3,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3df4a9-7789-4618-b974-3132ca10cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping from (1,3) to (3,)\n",
    "for i in range(len(target_angle_list_best)):\n",
    "    target_angle_list_best[i] = target_angle_list_best[i].reshape((3,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b429b84-e1e4-4396-9ed0-29f06984addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined PT (predicted, true)\n",
    "PT_value_best = [(PT_predicted, PT_true) for (PT_predicted,*_),(PT_true,*_) in zip(predicted_list_best,target_angle_list_best)]\n",
    "\n",
    "# Convert x-score value to original value\n",
    "for i in range(len(PT_value_best)):\n",
    "    temp = list(PT_value_best[i])\n",
    "    temp[0] = std_cobb_angle_test*temp[0] + mean_cobb_angle_test\n",
    "    temp[1] = std_cobb_angle_test*temp[1] + mean_cobb_angle_test\n",
    "    PT_value_best[i] = tuple(temp)\n",
    "\n",
    "# PT_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd81ec-cefd-4d79-a385-1f70da2eb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT_value_best = [(MT_predicted,MT_true) for (_,MT_predicted,_),(_,MT_true,_) in zip(predicted_list_best,target_angle_list_best)]\n",
    "\n",
    "# Convert x-score value to original value\n",
    "for i in range(len(MT_value_best)):\n",
    "    temp = list(MT_value_best[i])\n",
    "    temp[0] = std_cobb_angle_test*temp[0] + mean_cobb_angle_test\n",
    "    temp[1] = std_cobb_angle_test*temp[1] + mean_cobb_angle_test\n",
    "    MT_value_best[i] = tuple(temp)\n",
    "\n",
    "# MT_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55940ee8-828c-44c6-869a-cbafc76d09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TL_value_best = [(TL_predicted,TL_true) for (*_,TL_predicted),(*_,TL_true) in zip(predicted_list_best,target_angle_list_best)]\n",
    "\n",
    "# Convert x-score value to original value\n",
    "for i in range(len(TL_value_best)):\n",
    "    temp = list(TL_value_best[i])\n",
    "    temp[0] = std_cobb_angle_test*temp[0] + mean_cobb_angle_test\n",
    "    temp[1] = std_cobb_angle_test*temp[1] + mean_cobb_angle_test\n",
    "    TL_value_best[i] = tuple(temp)\n",
    "\n",
    "# TL_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1184b12-a622-4d52-887c-dbea22fa44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8900c-04f4-4c9c-8cf9-8a57c0722132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PT value\n",
    "\n",
    "predicted_pt = [item[0] for item in PT_value_best]\n",
    "true_pt = [item[1] for item in PT_value_best]\n",
    "\n",
    "plt.scatter(true_pt, predicted_pt, label='model prediction', color='#fc0505', s=5)\n",
    "plt.scatter(true_pt, true_pt, label='perfect prediction', color='blue', s=5)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('PT angle (lowest validation model)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad52da-a5c0-41ce-b4bf-7a7296134bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot MT value\n",
    "\n",
    "predicted_mt = [item[0] for item in MT_value_best]\n",
    "true_mt = [item[1] for item in MT_value_best]\n",
    "\n",
    "plt.scatter(true_mt, predicted_mt, label='model prediction', color='#fc0505', s=5)\n",
    "plt.scatter(true_mt, true_mt, label='perfect prediction', color='blue', s=5)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('MT angle (lowest validation model)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20613d9-0102-4193-a409-ae140de30315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot TL value\n",
    "\n",
    "predicted_tl = [item[0] for item in TL_value_best]\n",
    "true_tl = [item[1] for item in TL_value_best]\n",
    "\n",
    "plt.scatter(true_tl, predicted_tl, label='model predict', color='#fc0505', s=5)\n",
    "plt.scatter(true_tl, true_tl, label='perfect prediction', color='blue', s=5)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('TL angle (lowest validation model)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97436387-9b9f-4bdc-948e-81d0fca251e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33909e8c-2849-4de4-9268-85841a9415aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished model\n",
    "# PT\n",
    "true_PT_final = [element[1] for element in PT_value_final]\n",
    "pred_PT_final = [element[0] for element in PT_value_final]\n",
    "\n",
    "# MT\n",
    "true_MT_final = [element[1] for element in MT_value_final]\n",
    "pred_MT_final = [element[0] for element in MT_value_final]\n",
    "\n",
    "# TL\n",
    "true_TL_final = [element[1] for element in TL_value_final]\n",
    "pred_TL_final = [element[0] for element in TL_value_final]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4114d7dc-9bff-4188-993b-bb62cc6a5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "# PT\n",
    "true_PT_best = [element[1] for element in PT_value_best]\n",
    "pred_PT_best = [element[0] for element in PT_value_best]\n",
    "\n",
    "# MT\n",
    "true_MT_best = [element[1] for element in MT_value_best]\n",
    "pred_MT_best = [element[0] for element in MT_value_best]\n",
    "\n",
    "# TL\n",
    "true_TL_best = [element[1] for element in TL_value_best]\n",
    "pred_TL_best = [element[0] for element in TL_value_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5372e-7b69-461a-901c-c45b23fbef21",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### R Square/Adjusted R Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d18e5-aef0-482a-af03-8415f231a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d41840-74e0-4faf-bf59-e5166c2fd11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for finished_model\n",
    "\n",
    "\n",
    "# PT\n",
    "r2_PT_fin = r2_score(true_PT_final, pred_PT_final)\n",
    "print('R Square score for PT (finished model) ', r2_PT_fin)\n",
    "\n",
    "# MT\n",
    "r2_MT_fin = r2_score(true_MT_final, pred_MT_final)\n",
    "print('R Square score for MT (finished model) ', r2_MT_fin)\n",
    "\n",
    "# TL\n",
    "r2_TL_fin = r2_score(true_TL_final, pred_TL_final)\n",
    "print('R Square score for TL (finished model) ', r2_TL_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ab1bf-50be-481f-9f27-a63a2950e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the best model\n",
    "\n",
    "# PT\n",
    "r2_PT_best = r2_score(true_PT_best, pred_PT_best)\n",
    "print('R Square score for PT (best model) ', r2_PT_best)\n",
    "\n",
    "# MT\n",
    "r2_MT_best = r2_score(true_MT_best, pred_MT_best)\n",
    "print('R Square score for MT (best model) ', r2_MT_best)\n",
    "\n",
    "# TL\n",
    "r2_TL_best = r2_score(true_TL_best, pred_TL_best)\n",
    "print('R Square score for TL (best model) ', r2_TL_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bcf451-93f1-4f30-9538-8bd028311848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual calculation\n",
    "def r2(true, pred):\n",
    "    true_mean = sum(true) / len(true)\n",
    "    \n",
    "    ssr = sum((true[i] - pred[i])**2 for i in range(len(true)))\n",
    "    sst = sum((true[i] - true_mean)**2 for i in range(len(true)))\n",
    "    r_squared = 1 - (ssr / sst)\n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9c4d5-ced6-437e-a7e8-f4d9c3cf5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2(true_MT_best,pred_MT_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b779e-f10e-4c0c-9594-3ff825fa9ce7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Residual Standard Error of the Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b217298-f633-4825-9ef5-1e5241830550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def RSE(true, predict):\n",
    "\n",
    "    # Calculate the residuals\n",
    "    residuals = np.array(true) - np.array(predict)\n",
    "\n",
    "    # Compute the sum of squared residuals\n",
    "    ssr = np.sum(residuals**2)\n",
    "\n",
    "    # Calculate the RSE\n",
    "    n = len(true)  # Total number of observations\n",
    "    p = 1  # Number of model parameters (including intercept, if applicable)\n",
    "    rse = np.sqrt(ssr / (n - p))\n",
    "\n",
    "    return rse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e26a8-c176-4b10-94b0-5e2b531e4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for finished model\n",
    "# PT\n",
    "print('RSE PT (finished) ', RSE(true_PT_final, pred_PT_final))\n",
    "\n",
    "# MT\n",
    "print('RSE MT (finished) ', RSE(true_MT_final, pred_MT_final))\n",
    "\n",
    "# TL\n",
    "print('RSE TL (finished) ', RSE(true_TL_final, pred_TL_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78631f40-8cd8-4b7e-a632-3c0190daa4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for best model\n",
    "# PT\n",
    "print('RSE PT (lowest_validation) ', RSE(true_PT_best, pred_PT_best))\n",
    "\n",
    "# MT\n",
    "print('RSE MT (lowest_validation) ', RSE(true_MT_best, pred_MT_best))\n",
    "\n",
    "# TL\n",
    "print('RSE TL (lowest_validation) ', RSE(true_TL_best, pred_TL_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f65ce4-a356-4592-b6ea-9a007ff42e74",
   "metadata": {},
   "source": [
    "##### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd501afa-e15e-47f5-aa8b-068852cda73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78425741-1fef-4087-bfd2-8374cb9d4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p_val = stats.pearsonr(true_PT_best, pred_PT_best)\n",
    "print('Pearson Correlation r for PT angle: ', r)\n",
    "print('p_val: ', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990eb2da-ea22-4886-9400-1ea2a7afd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p_val = stats.pearsonr(true_MT_best, pred_MT_best)\n",
    "print('Pearson Correlation r for MT angle: ', r)\n",
    "print('p_val: ', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0955a-8ed9-4365-b9b6-876a14587cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p_val = stats.pearsonr(true_TL_best, pred_TL_best)\n",
    "print('Pearson Correlation r for TL angle: ', r)\n",
    "print('p_val: ', p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b2886-a2bc-4caa-881e-3504a35d7a7a",
   "metadata": {},
   "source": [
    "##### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38783434-a2af-4f9a-9bce-235673a4d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "# for lowest validation model\n",
    "rmse_pt = math.sqrt(mean_squared_error(true_PT_best, pred_PT_best))\n",
    "rmse_mt = math.sqrt(mean_squared_error(true_MT_best, pred_MT_best))\n",
    "rmse_tl = math.sqrt(mean_squared_error(true_TL_best, pred_TL_best))\n",
    "\n",
    "print('RMSE PT (lowest_validation_model) ', rmse_pt)\n",
    "print('RMSE MT (lowest_validation_model) ', rmse_mt)\n",
    "print('RMSE TL (lowest_validation_model) ', rmse_tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abd9fc-1451-4c00-b042-9296261f484f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b939779-5a68-4118-8f8d-ebe94e3d651b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7369329-aedd-435d-85e1-ed04dc2eb514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
